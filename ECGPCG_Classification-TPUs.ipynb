{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting for training on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TPU_NAME'] = 'local'\n",
    "\n",
    "os.environ['NEXT_PLUGGABLE_DEVICE_USE_C_API'] = 'true'\n",
    "os.environ['TF_PLUGGABLE_DEVICE_LIBRARY_PATH'] = 'local'\n",
    "os.environ['TF_PLUGGABLE_DEVICE_LIBRARY_PATH'] = '/lib/libtpu.so'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.tpu.topology.Topology at 0x7f0b5cef3160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.TPUStrategy(cluster_resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All devices:  [LogicalDevice(name='/device:TPU:0', device_type='TPU'), LogicalDevice(name='/device:TPU:1', device_type='TPU'), LogicalDevice(name='/device:TPU:2', device_type='TPU'), LogicalDevice(name='/device:TPU:3', device_type='TPU'), LogicalDevice(name='/device:TPU:4', device_type='TPU'), LogicalDevice(name='/device:TPU:5', device_type='TPU'), LogicalDevice(name='/device:TPU:6', device_type='TPU'), LogicalDevice(name='/device:TPU:7', device_type='TPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(10000, 2)),\n",
    "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "#         tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(128, activation='relu'),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(6, activation='softmax')\n",
    "#     ])\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(10000, 2)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')  # 6 classes for exercise intensity levels\n",
    "    ])\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "    model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),  # Set your desired learning rate\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d/kernel:0 is assigned to /job:localhost/replica:0/task:0/device:TPU:0\n",
      "conv1d/bias:0 is assigned to /job:localhost/replica:0/task:0/device:TPU:0\n",
      "conv1d_1/kernel:0 is assigned to /job:localhost/replica:0/task:0/device:TPU:0\n",
      "conv1d_1/bias:0 is assigned to /job:localhost/replica:0/task:0/device:TPU:0\n",
      "dense/kernel:0 is assigned to /job:localhost/replica:0/task:0/device:TPU:0\n",
      "dense/bias:0 is assigned to /job:localhost/replica:0/task:0/device:TPU:0\n",
      "dense_1/kernel:0 is assigned to /job:localhost/replica:0/task:0/device:TPU:0\n",
      "dense_1/bias:0 is assigned to /job:localhost/replica:0/task:0/device:TPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check the device assignment for operations\n",
    "for layer in model.layers:\n",
    "    for weight in layer.weights:\n",
    "        print(f\"{weight.name} is assigned to {weight.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# Paths to files\n",
    "mat_files_directory = \"./physionet.org/files/ephnogram/1.0.0/MAT/\"\n",
    "pandas_csv_file_path = \"./physionet.org/files/ephnogram/1.0.0/ECGPCGSpreadsheet.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from 'recording scenario' to label (1 to 6)\n",
    "scenario_to_label = {\n",
    "    'rest: laying on bed': 1,\n",
    "    'rest: sitting on armchair': 2,\n",
    "    'exercise: walking at constant speed': 3,\n",
    "    'exercise: pedaling a stationary bicycle': 4,\n",
    "    'exercise: bicycle stress test': 5,\n",
    "    'exercise: bruce protocol treadmill stress test': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "### Maping file to labels\n",
    "df = pd.read_csv(pandas_csv_file_path)\n",
    "\n",
    "# Create a dictionary mapping filenames to labels\n",
    "file_to_label = {}\n",
    "for index, row in df.iterrows():\n",
    "    filename = row['Record Name']  # Adjust column name if necessary\n",
    "    scenario = row['Recording Scenario']  # Adjust column name if necessary\n",
    "    # Check if the scenario can be mapped to a label\n",
    "    if type(scenario) == str:\n",
    "        if scenario.lower() in scenario_to_label:\n",
    "            label = scenario_to_label[scenario.lower()]\n",
    "            file_to_label[filename] = label\n",
    "    else:\n",
    "        # Skip scenarios that don't map to a clear label\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(ecg_signal, pcg_signal):\n",
    "    # Normalize ECG and PCG signals\n",
    "    ecg_signal = (ecg_signal - np.min(ecg_signal)) / (np.max(ecg_signal) - np.min(ecg_signal))\n",
    "    pcg_signal = (pcg_signal - np.min(pcg_signal)) / (np.max(pcg_signal) - np.min(pcg_signal))\n",
    "\n",
    "    # Combine ECG and PCG signals into one dataset\n",
    "    combined_signal = np.stack((ecg_signal, pcg_signal), axis=-1)\n",
    "\n",
    "    return combined_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "def process_mat_file(file_path, label):\n",
    "    # Load the .mat file\n",
    "    mat_data = scipy.io.loadmat(file_path)\n",
    "    \n",
    "    # Extract first channel of ECG and the only channel of PCG\n",
    "    ecg_data = mat_data['ECG'][0]  # Taking only the first channel of ECG\n",
    "    pcg_data = mat_data['PCG'][0]  # Assuming only one channel for PCG\n",
    "    \n",
    "    # Preprocess the signals\n",
    "    combined_signal = preprocess_data(ecg_data, pcg_data)\n",
    "    \n",
    "    # Number of points in the signal\n",
    "    num_points = combined_signal.shape[0]\n",
    "    \n",
    "    # Split data into segments of 10,000 points\n",
    "    samples = []\n",
    "    for i in range(0, num_points, 10000):\n",
    "        if i + 10000 <= num_points:\n",
    "            sample = combined_signal[i:i+10000]\n",
    "            samples.append((sample, label))\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_single_file(args):\n",
    "#     file_path, label = args\n",
    "#     samples = process_mat_file(file_path, label)\n",
    "#     return samples\n",
    "\n",
    "# def process_files_in_parallel(mat_files_directory, file_to_label):\n",
    "#     all_samples = []\n",
    "#     mat_files = [f for f in os.listdir(mat_files_directory) if f.endswith('.mat')]\n",
    "    \n",
    "#     # Prepare the arguments for each file\n",
    "#     args_list = []\n",
    "#     for mat_file in mat_files:\n",
    "#         file_path = os.path.join(mat_files_directory, mat_file)\n",
    "#         name = mat_file.split('.')[0]\n",
    "#         if name in file_to_label:\n",
    "#             label = file_to_label[name]\n",
    "#             args_list.append((file_path, label))\n",
    "\n",
    "#     # Use multiprocessing to process files in parallel\n",
    "#     with Pool(processes=70) as pool:\n",
    "#         results = pool.map(process_single_file, args_list)\n",
    "    \n",
    "#     # Combine the results\n",
    "#     for result in results:\n",
    "#         all_samples.extend(result)\n",
    "    \n",
    "#     return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# all_samples = process_files_in_parallel(mat_files_directory, file_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ECGPCG0006.mat', 'ECGPCG0013.mat', 'ECGPCG0023.mat', 'ECGPCG0016.mat', 'ECGPCG0053.mat', 'ECGPCG0039.mat', 'ECGPCG0015.mat', 'ECGPCG0010.mat', 'ECGPCG0028.mat', 'ECGPCG0051.mat', 'ECGPCG0008.mat', 'ECGPCG0046.mat', 'ECGPCG0067.mat', 'ECGPCG0011.mat', 'ECGPCG0063.mat', 'ECGPCG0040.mat', 'ECGPCG0068.mat', 'ECGPCG0002.mat', 'ECGPCG0050.mat', 'ECGPCG0009.mat', 'ECGPCG0029.mat', 'ECGPCG0032.mat', 'ECGPCG0037.mat', 'ECGPCG0066.mat', 'ECGPCG0027.mat', 'ECGPCG0014.mat', 'ECGPCG0054.mat', 'ECGPCG0026.mat', 'ECGPCG0060.mat', 'ECGPCG0062.mat', 'ECGPCG0024.mat', 'ECGPCG0055.mat', 'ECGPCG0004.mat', 'ECGPCG0030.mat', 'ECGPCG0031.mat', 'ECGPCG0045.mat', 'ECGPCG0012.mat', 'ECGPCG0047.mat', 'ECGPCG0069.mat', 'ECGPCG0044.mat', 'ECGPCG0057.mat', 'ECGPCG0005.mat', 'ECGPCG0025.mat', 'ECGPCG0003.mat', 'ECGPCG0042.mat', 'ECGPCG0052.mat', 'ECGPCG0049.mat', 'ECGPCG0018.mat', 'ECGPCG0038.mat', 'ECGPCG0034.mat', 'ECGPCG0048.mat', 'ECGPCG0017.mat', 'ECGPCG0056.mat', 'ECGPCG0019.mat', 'ECGPCG0001.mat', 'ECGPCG0021.mat', 'ECGPCG0065.mat', 'ECGPCG0058.mat', 'ECGPCG0036.mat', 'ECGPCG0041.mat', 'ECGPCG0035.mat', 'ECGPCG0043.mat', 'ECGPCG0061.mat', 'ECGPCG0033.mat', 'ECGPCG0007.mat', 'ECGPCG0064.mat', 'ECGPCG0022.mat', 'ECGPCG0020.mat', 'ECGPCG0059.mat']\n"
     ]
    }
   ],
   "source": [
    "# Process each .mat file and assign labels\n",
    "all_samples = []\n",
    "mat_files = [f for f in os.listdir(mat_files_directory) if f.endswith('.mat')]\n",
    "print(mat_files)\n",
    "for mat_file in mat_files:\n",
    "    file_path = os.path.join(mat_files_directory, mat_file)\n",
    "    name = mat_file.split('.')[0]\n",
    "    if name in file_to_label:\n",
    "        label = file_to_label[name]\n",
    "\n",
    "        samples = process_mat_file(file_path, label)\n",
    "        all_samples.extend(samples)  # Store all samples in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 64992, Example sample label: 2\n"
     ]
    }
   ],
   "source": [
    "# Example: Print the number of samples and a sample label\n",
    "example_sample = all_samples[0] if all_samples else None\n",
    "if example_sample:\n",
    "    print(f\"Number of samples: {len(all_samples)}, Example sample label: {example_sample[1]}\")\n",
    "else:\n",
    "    print(\"No samples were processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (58492, 10000, 2)\n",
      "X_test shape: (6500, 10000, 2)\n",
      "y_train shape: (58492, 6)\n",
      "y_test shape: (6500, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def create_datasets(all_samples, test_size=0.2):\n",
    "    # Separate signals and labels from the all_samples list\n",
    "    signals = [sample[0] for sample in all_samples]\n",
    "    labels = [sample[1] for sample in all_samples]\n",
    "    \n",
    "    # Convert lists to numpy arrays for better performance\n",
    "    signals = np.array(signals)\n",
    "    labels = np.array(labels)\n",
    "    labels = to_categorical(labels - 1, num_classes=6)\n",
    "    # Use train_test_split to split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(signals, labels, test_size=test_size, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Example usage\n",
    "X_train, X_test, y_train, y_test = create_datasets(all_samples, test_size=0.1)\n",
    "\n",
    "# Output the shapes of the created datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample ,y_train_sample = X_train[:1024], y_train[:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### part of convert to training on TPUs\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepair dataset for Training on TPU\n",
    "batch_size = 512\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 10000, 2), dtype=tf.float64, name=None), TensorSpec(shape=(None, 6), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset element is on: /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "for element in test_dataset:\n",
    "    print(f\"Dataset element is on: {element[0].device}\")\n",
    "    break  # Check the placement of just one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "115/115 [==============================] - 304s 3s/step - loss: 1.1020 - accuracy: 0.5605 - val_loss: 1.3375 - val_accuracy: 0.4855\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 1.0957 - accuracy: 0.5561 - val_loss: 1.7936 - val_accuracy: 0.3620\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 1.0384 - accuracy: 0.5747 - val_loss: 1.0848 - val_accuracy: 0.5543\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.9865 - accuracy: 0.5965 - val_loss: 1.1386 - val_accuracy: 0.5315\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.9617 - accuracy: 0.6080 - val_loss: 1.1872 - val_accuracy: 0.5420\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 1.0451 - accuracy: 0.5823 - val_loss: 1.6271 - val_accuracy: 0.3715\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 1.0324 - accuracy: 0.5894 - val_loss: 1.3137 - val_accuracy: 0.4766\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 1.0298 - accuracy: 0.5873 - val_loss: 2.8549 - val_accuracy: 0.2986\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.9695 - accuracy: 0.6159 - val_loss: 0.9914 - val_accuracy: 0.5878\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.9338 - accuracy: 0.6254 - val_loss: 0.8764 - val_accuracy: 0.6337\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.9014 - accuracy: 0.6296 - val_loss: 0.8692 - val_accuracy: 0.6292\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.8867 - accuracy: 0.6346 - val_loss: 1.6626 - val_accuracy: 0.4177\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.8768 - accuracy: 0.6428 - val_loss: 0.8140 - val_accuracy: 0.6645\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.8450 - accuracy: 0.6655 - val_loss: 0.7969 - val_accuracy: 0.6740\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.8154 - accuracy: 0.6793 - val_loss: 0.8198 - val_accuracy: 0.6712\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.8039 - accuracy: 0.6831 - val_loss: 1.7930 - val_accuracy: 0.4288\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.8119 - accuracy: 0.6792 - val_loss: 0.7904 - val_accuracy: 0.6794\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.8034 - accuracy: 0.6856 - val_loss: 0.7991 - val_accuracy: 0.6585\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7853 - accuracy: 0.6871 - val_loss: 0.7403 - val_accuracy: 0.6938\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7699 - accuracy: 0.6928 - val_loss: 0.8630 - val_accuracy: 0.6285\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7551 - accuracy: 0.7043 - val_loss: 0.7655 - val_accuracy: 0.6877\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7835 - accuracy: 0.6860 - val_loss: 3.0828 - val_accuracy: 0.3652\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.8009 - accuracy: 0.6827 - val_loss: 0.8877 - val_accuracy: 0.6286\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.7712 - accuracy: 0.6989 - val_loss: 0.8536 - val_accuracy: 0.6692\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.7476 - accuracy: 0.7066 - val_loss: 0.7148 - val_accuracy: 0.7063\n",
      "Epoch 26/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7319 - accuracy: 0.7099 - val_loss: 0.9015 - val_accuracy: 0.6402\n",
      "Epoch 27/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7333 - accuracy: 0.7070 - val_loss: 0.8827 - val_accuracy: 0.6392\n",
      "Epoch 28/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7261 - accuracy: 0.7099 - val_loss: 0.7936 - val_accuracy: 0.6706\n",
      "Epoch 29/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7241 - accuracy: 0.7117 - val_loss: 0.7107 - val_accuracy: 0.7142\n",
      "Epoch 30/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.7045 - accuracy: 0.7194 - val_loss: 1.0547 - val_accuracy: 0.5598\n",
      "Epoch 31/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.6913 - accuracy: 0.7256 - val_loss: 0.6407 - val_accuracy: 0.7331\n",
      "Epoch 32/50\n",
      "115/115 [==============================] - 304s 3s/step - loss: 0.6771 - accuracy: 0.7337 - val_loss: 0.8762 - val_accuracy: 0.6389\n",
      "Epoch 33/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.6808 - accuracy: 0.7278 - val_loss: 1.7844 - val_accuracy: 0.4094\n",
      "Epoch 34/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.6842 - accuracy: 0.7170 - val_loss: 0.6582 - val_accuracy: 0.7038\n",
      "Epoch 35/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.6509 - accuracy: 0.7367 - val_loss: 0.7270 - val_accuracy: 0.6912\n",
      "Epoch 36/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.6458 - accuracy: 0.7439 - val_loss: 0.9671 - val_accuracy: 0.6009\n",
      "Epoch 37/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.8407 - accuracy: 0.6666 - val_loss: 1.7411 - val_accuracy: 0.4312\n",
      "Epoch 38/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.6589 - accuracy: 0.7404 - val_loss: 0.5929 - val_accuracy: 0.7562\n",
      "Epoch 40/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.6416 - accuracy: 0.7432 - val_loss: 1.2964 - val_accuracy: 0.5192\n",
      "Epoch 41/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.6622 - accuracy: 0.7367 - val_loss: 0.8375 - val_accuracy: 0.6532\n",
      "Epoch 42/50\n",
      "115/115 [==============================] - 302s 3s/step - loss: 0.6192 - accuracy: 0.7515 - val_loss: 0.5562 - val_accuracy: 0.7762\n",
      "Epoch 43/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.6002 - accuracy: 0.7609 - val_loss: 0.5525 - val_accuracy: 0.7697\n",
      "Epoch 44/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.5957 - accuracy: 0.7589 - val_loss: 1.1342 - val_accuracy: 0.6218\n",
      "Epoch 45/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.5988 - accuracy: 0.7584 - val_loss: 0.5974 - val_accuracy: 0.7531\n",
      "Epoch 46/50\n",
      "115/115 [==============================] - 303s 3s/step - loss: 0.6021 - accuracy: 0.7569 - val_loss: 1.0133 - val_accuracy: 0.5778\n",
      "Epoch 47/50\n",
      " 86/115 [=====================>........] - ETA: 1:16 - loss: 0.5843 - accuracy: 0.7653"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = 58492//batch_size\n",
    "validation_steps = 6500//batch_size\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          epochs=50,\n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "          validation_data=test_dataset, \n",
    "#           validation_steps=validation_steps\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " 50/115 [============>.................] - ETA: 2:50 - loss: 0.5320 - accuracy: 0.7855"
     ]
    }
   ],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model.fit(train_dataset,\n",
    "          epochs=60,\n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "          validation_data=test_dataset,\n",
    "          callbacks=[tensorboard_callback]\n",
    "#           validation_steps=validation_steps\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)\n",
    "model.save('ecg_pcg_tpu_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
