{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "     |████████████████████████████████| 12.4 MB 14.7 MB/s            \n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "     |████████████████████████████████| 229 kB 116.3 MB/s            \n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "     |████████████████████████████████| 345 kB 129.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, python-dateutil, pandas\n",
      "Successfully installed pandas-2.0.3 python-dateutil-2.9.0.post0 tzdata-2024.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "     |████████████████████████████████| 11.1 MB 14.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "     |████████████████████████████████| 301 kB 105.9 MB/s            \n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.3.2 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files\n",
    "mat_files_directory = \"./physionet.org/files/ephnogram/1.0.0/MAT/\"\n",
    "pandas_csv_file_path = \"./physionet.org/files/ephnogram/1.0.0/ECGPCGSpreadsheet.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from 'recording scenario' to label (1 to 6)\n",
    "scenario_to_label = {\n",
    "    'rest: laying on bed': 1,\n",
    "    'rest: sitting on armchair': 2,\n",
    "    'exercise: walking at constant speed': 3,\n",
    "    'exercise: pedaling a stationary bicycle': 4,\n",
    "    'exercise: bicycle stress test': 5,\n",
    "    'exercise: bruce protocol treadmill stress test': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "### Maping file to labels\n",
    "df = pd.read_csv(pandas_csv_file_path)\n",
    "\n",
    "# Create a dictionary mapping filenames to labels\n",
    "file_to_label = {}\n",
    "for index, row in df.iterrows():\n",
    "    filename = row['Record Name']  # Adjust column name if necessary\n",
    "    scenario = row['Recording Scenario']  # Adjust column name if necessary\n",
    "    # Check if the scenario can be mapped to a label\n",
    "    if type(scenario) == str:\n",
    "        if scenario.lower() in scenario_to_label:\n",
    "            label = scenario_to_label[scenario.lower()]\n",
    "            file_to_label[filename] = label\n",
    "    else:\n",
    "        # Skip scenarios that don't map to a clear label\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ECGPCG0001': 4,\n",
       " 'ECGPCG0002': 4,\n",
       " 'ECGPCG0003': 2,\n",
       " 'ECGPCG0004': 2,\n",
       " 'ECGPCG0005': 2,\n",
       " 'ECGPCG0006': 2,\n",
       " 'ECGPCG0007': 2,\n",
       " 'ECGPCG0008': 2,\n",
       " 'ECGPCG0009': 2,\n",
       " 'ECGPCG0010': 2,\n",
       " 'ECGPCG0011': 2,\n",
       " 'ECGPCG0012': 2,\n",
       " 'ECGPCG0013': 1,\n",
       " 'ECGPCG0014': 1,\n",
       " 'ECGPCG0015': 1,\n",
       " 'ECGPCG0016': 1,\n",
       " 'ECGPCG0020': 1,\n",
       " 'ECGPCG0021': 1,\n",
       " 'ECGPCG0022': 1,\n",
       " 'ECGPCG0023': 1,\n",
       " 'ECGPCG0024': 4,\n",
       " 'ECGPCG0025': 4,\n",
       " 'ECGPCG0026': 4,\n",
       " 'ECGPCG0027': 4,\n",
       " 'ECGPCG0028': 4,\n",
       " 'ECGPCG0029': 4,\n",
       " 'ECGPCG0030': 4,\n",
       " 'ECGPCG0031': 4,\n",
       " 'ECGPCG0032': 4,\n",
       " 'ECGPCG0033': 4,\n",
       " 'ECGPCG0034': 4,\n",
       " 'ECGPCG0035': 6,\n",
       " 'ECGPCG0036': 6,\n",
       " 'ECGPCG0037': 6,\n",
       " 'ECGPCG0038': 6,\n",
       " 'ECGPCG0039': 6,\n",
       " 'ECGPCG0040': 2,\n",
       " 'ECGPCG0046': 6,\n",
       " 'ECGPCG0047': 6,\n",
       " 'ECGPCG0052': 6,\n",
       " 'ECGPCG0054': 6,\n",
       " 'ECGPCG0055': 6,\n",
       " 'ECGPCG0056': 6,\n",
       " 'ECGPCG0059': 5,\n",
       " 'ECGPCG0060': 5,\n",
       " 'ECGPCG0061': 5,\n",
       " 'ECGPCG0062': 5,\n",
       " 'ECGPCG0064': 5,\n",
       " 'ECGPCG0065': 5,\n",
       " 'ECGPCG0066': 5,\n",
       " 'ECGPCG0067': 5,\n",
       " 'ECGPCG0068': 5,\n",
       " 'ECGPCG0069': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(ecg_signal, pcg_signal):\n",
    "    # Normalize ECG and PCG signals\n",
    "    ecg_signal = (ecg_signal - np.min(ecg_signal)) / (np.max(ecg_signal) - np.min(ecg_signal))\n",
    "    pcg_signal = (pcg_signal - np.min(pcg_signal)) / (np.max(pcg_signal) - np.min(pcg_signal))\n",
    "\n",
    "    # Combine ECG and PCG signals into one dataset\n",
    "    combined_signal = np.stack((ecg_signal, pcg_signal), axis=-1)\n",
    "\n",
    "    return combined_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mat_file(file_path, label):\n",
    "    # Load the .mat file\n",
    "    mat_data = scipy.io.loadmat(file_path)\n",
    "    \n",
    "    # Extract first channel of ECG and the only channel of PCG\n",
    "    ecg_data = mat_data['ECG'][0]  # Taking only the first channel of ECG\n",
    "    pcg_data = mat_data['PCG'][0]  # Assuming only one channel for PCG\n",
    "    \n",
    "    # Preprocess the signals\n",
    "    combined_signal = preprocess_data(ecg_data, pcg_data)\n",
    "    \n",
    "    # Number of points in the signal\n",
    "    num_points = combined_signal.shape[0]\n",
    "    \n",
    "    # Split data into segments of 10,000 points\n",
    "    samples = []\n",
    "    for i in range(0, num_points, 10000):\n",
    "        if i + 10000 <= num_points:\n",
    "            sample = combined_signal[i:i+10000]\n",
    "            samples.append((sample, label))\n",
    "    \n",
    "    return samples\n",
    "    # Split data into segments of 10,000 points\n",
    "#     samples = []\n",
    "#     for i in range(0, num_points, 10000):\n",
    "#         if i + 10000 <= num_points:\n",
    "#             ecg_sample = ecg_data[i:i+10000]\n",
    "#             pcg_sample = pcg_data[i:i+10000]\n",
    "#             samples.append((ecg_sample, pcg_sample, label))\n",
    "    \n",
    "#     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(args):\n",
    "    file_path, label = args\n",
    "    samples = process_mat_file(file_path, label)\n",
    "    return samples\n",
    "\n",
    "def process_files_in_parallel(mat_files_directory, file_to_label):\n",
    "    all_samples = []\n",
    "    mat_files = [f for f in os.listdir(mat_files_directory) if f.endswith('.mat')]\n",
    "    \n",
    "    # Prepare the arguments for each file\n",
    "    args_list = []\n",
    "    for mat_file in mat_files:\n",
    "        file_path = os.path.join(mat_files_directory, mat_file)\n",
    "        name = mat_file.split('.')[0]\n",
    "        if name in file_to_label:\n",
    "            label = file_to_label[name]\n",
    "            args_list.append((file_path, label))\n",
    "\n",
    "    # Use multiprocessing to process files in parallel\n",
    "    with Pool(processes=70) as pool:\n",
    "        results = pool.map(process_single_file, args_list)\n",
    "    \n",
    "    # Combine the results\n",
    "    for result in results:\n",
    "        all_samples.extend(result)\n",
    "    \n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "all_samples = process_files_in_parallel(mat_files_directory, file_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ECGPCG0006.mat', 'ECGPCG0013.mat', 'ECGPCG0023.mat', 'ECGPCG0016.mat', 'ECGPCG0053.mat', 'ECGPCG0039.mat', 'ECGPCG0015.mat', 'ECGPCG0010.mat', 'ECGPCG0028.mat', 'ECGPCG0051.mat', 'ECGPCG0008.mat', 'ECGPCG0046.mat', 'ECGPCG0067.mat', 'ECGPCG0011.mat', 'ECGPCG0063.mat', 'ECGPCG0040.mat', 'ECGPCG0068.mat', 'ECGPCG0002.mat', 'ECGPCG0050.mat', 'ECGPCG0009.mat', 'ECGPCG0029.mat', 'ECGPCG0032.mat', 'ECGPCG0037.mat', 'ECGPCG0066.mat', 'ECGPCG0027.mat', 'ECGPCG0014.mat', 'ECGPCG0054.mat', 'ECGPCG0026.mat', 'ECGPCG0060.mat', 'ECGPCG0062.mat', 'ECGPCG0024.mat', 'ECGPCG0055.mat', 'ECGPCG0004.mat', 'ECGPCG0030.mat', 'ECGPCG0031.mat', 'ECGPCG0045.mat', 'ECGPCG0012.mat', 'ECGPCG0047.mat', 'ECGPCG0069.mat', 'ECGPCG0044.mat', 'ECGPCG0057.mat', 'ECGPCG0005.mat', 'ECGPCG0025.mat', 'ECGPCG0003.mat', 'ECGPCG0042.mat', 'ECGPCG0052.mat', 'ECGPCG0049.mat', 'ECGPCG0018.mat', 'ECGPCG0038.mat', 'ECGPCG0034.mat', 'ECGPCG0048.mat', 'ECGPCG0017.mat', 'ECGPCG0056.mat', 'ECGPCG0019.mat', 'ECGPCG0001.mat', 'ECGPCG0021.mat', 'ECGPCG0065.mat', 'ECGPCG0058.mat', 'ECGPCG0036.mat', 'ECGPCG0041.mat', 'ECGPCG0035.mat', 'ECGPCG0043.mat', 'ECGPCG0061.mat', 'ECGPCG0033.mat', 'ECGPCG0007.mat', 'ECGPCG0064.mat', 'ECGPCG0022.mat', 'ECGPCG0020.mat', 'ECGPCG0059.mat']\n"
     ]
    }
   ],
   "source": [
    "# # Process each .mat file and assign labels\n",
    "# all_samples = []\n",
    "# mat_files = [f for f in os.listdir(mat_files_directory) if f.endswith('.mat')]\n",
    "# print(mat_files)\n",
    "# for mat_file in mat_files:\n",
    "#     file_path = os.path.join(mat_files_directory, mat_file)\n",
    "#     name = mat_file.split('.')[0]\n",
    "#     if name in file_to_label:\n",
    "#         label = file_to_label[name]\n",
    "\n",
    "#         samples = process_mat_file(file_path, label)\n",
    "#         all_samples.extend(samples)  # Store all samples in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 64992, Example sample label: 2\n"
     ]
    }
   ],
   "source": [
    "# Example: Print the number of samples and a sample label\n",
    "example_sample = all_samples[0] if all_samples else None\n",
    "if example_sample:\n",
    "    print(f\"Number of samples: {len(all_samples)}, Example sample label: {example_sample[1]}\")\n",
    "else:\n",
    "    print(\"No samples were processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (58492, 10000, 2)\n",
      "X_test shape: (6500, 10000, 2)\n",
      "y_train shape: (58492,)\n",
      "y_test shape: (6500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_samples is a list of tuples, where each tuple is (signal, label)\n",
    "# Example structure: [(np.array(10000, 2), label), ...]\n",
    "\n",
    "def create_datasets(all_samples, test_size=0.2):\n",
    "    # Separate signals and labels from the all_samples list\n",
    "    signals = [sample[0] for sample in all_samples]\n",
    "    labels = [sample[1] for sample in all_samples]\n",
    "    \n",
    "    # Convert lists to numpy arrays for better performance\n",
    "    signals = np.array(signals)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Use train_test_split to split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(signals, labels, test_size=test_size, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Example usage\n",
    "X_train, X_test, y_train, y_test = create_datasets(all_samples, test_size=0.1)\n",
    "\n",
    "# Output the shapes of the created datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_train y_test to binary\n",
    "y_train_categorical = to_categorical(y_train - 1, num_classes=6)\n",
    "y_test_categorical = to_categorical(y_test - 1, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build model:\n",
    "\n",
    "# Step 4: Build CNN-LSTM Model\n",
    "def build_cnn_lstm(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN layers for feature extraction\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # LSTM layer for capturing temporal dependencies\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "\n",
    "    # Fully connected layers for classification\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6, activation='softmax'))  # 6 classes for exercise intensity levels\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Train the CNN-LSTM Model\n",
    "def train_cnn_lstm_model(model, X_train, y_train, X_test, y_test, epochs=50, batch_size=32):\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n",
    "    return history\n",
    "\n",
    "# Step 6: Classification\n",
    "def classify_exercise_intensity(model, X_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    return np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 7: Evaluate and Display Results\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = classify_exercise_intensity(model, X_test)\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58492, 10000, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 509/1828 [=======>......................] - ETA: 26:27 - loss: 1.4363 - accuracy: 0.3363"
     ]
    }
   ],
   "source": [
    "# Build and train the CNN-LSTM model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_cnn_lstm(input_shape)\n",
    "train_cnn_lstm_model(model, X_train, y_train_categorical, X_test, y_test_categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, X_test, y_test_categorical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
